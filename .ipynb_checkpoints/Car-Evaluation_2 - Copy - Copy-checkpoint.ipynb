{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "04b8dc55-935f-4dbe-a978-e7505c5d3407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd31d67-9fdc-4f9d-b113-d2091cf41500",
   "metadata": {},
   "source": [
    "__Importing the Car Evaluation Dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "63cdbea0-3c0f-43c0-9476-e78c31ff52a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "car_evaluation = fetch_ucirepo(id=19) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = car_evaluation.data.features \n",
    "y = car_evaluation.data.targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6270681a-0067-4fc4-b1fe-527dc65ef018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(1728, 6)\n",
      "(1728, 1)\n"
     ]
    }
   ],
   "source": [
    "print(type(X))\n",
    "print(type(y))\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccb4b1f-9e7b-4444-baaa-7f9e170833bd",
   "metadata": {},
   "source": [
    "__One Hot Encoding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "986e15d7-57c7-4db0-b08d-374c89539dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(1728, 21)\n",
      "(1728, 4)\n"
     ]
    }
   ],
   "source": [
    "X_encoded = pd.get_dummies(X) # One hot encoding\n",
    "y_encoded = pd.get_dummies(y) # One hot encoding\n",
    "\n",
    "# print(X_encoded)\n",
    "# print(y_encoded)\n",
    "\n",
    "print(type(X))\n",
    "print(type(y))\n",
    "\n",
    "print(X_encoded.shape)\n",
    "print(y_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098c3739-2b76-49a8-855d-dd581d7e92ca",
   "metadata": {},
   "source": [
    "__Dataset Partitioning__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "30a0b76c-11c1-42ec-b3e4-52452cc2a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# data split, 70% training and 30% temp (temp = validation + test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_encoded, y_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# 30% temp data into 15% validation and 15% test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ca26ff-3bb0-4065-8034-8ae886e8ed93",
   "metadata": {},
   "source": [
    "__Building a Single Decision Tree__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "394fe74e-e915-4cdc-add3-99ea460ed410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    # Initialize confusion matrix counts\n",
    "    acc_acc = acc_good = acc_unacc = acc_vgood = 0\n",
    "    good_acc = good_good = good_unacc = good_vgood = 0\n",
    "    unacc_acc = unacc_good = unacc_unacc = unacc_vgood = 0\n",
    "    vgood_acc = vgood_good = vgood_unacc = vgood_vgood = 0\n",
    "\n",
    "    y_true = y_true.to_numpy()\n",
    "    \n",
    "    # # Calculate confusion matrix manually\n",
    "    # for actual, predicted in zip(y_true, y_pred):\n",
    "    for i in range(len(y_true)):\n",
    "        actual = y_true[i]\n",
    "        # actual = y_true.iloc[i].values # <class 'pandas.core.frame.DataFrame'>\n",
    "        predicted = y_pred[i] # <class 'numpy.ndarray'>\n",
    "        \n",
    "        if actual[0] and predicted[0]:\n",
    "            acc_acc += 1\n",
    "        elif actual[0] and predicted[1]:\n",
    "            acc_good += 1\n",
    "        elif actual[0] and predicted[2]:\n",
    "            acc_unacc += 1\n",
    "        elif actual[0] and predicted[3]:\n",
    "            acc_vgood += 1\n",
    "        elif actual[1] and predicted[0]:\n",
    "            good_acc += 1\n",
    "        elif actual[1] and predicted[1]:\n",
    "            good_good += 1\n",
    "        elif actual[1] and predicted[2]:\n",
    "            good_unacc += 1\n",
    "        elif actual[1] and predicted[3]:\n",
    "            good_vgood += 1\n",
    "        elif actual[2] and predicted[0]:\n",
    "            unacc_acc += 1\n",
    "        elif actual[2] and predicted[1]:\n",
    "            unacc_good += 1\n",
    "        elif actual[2] and predicted[2]:\n",
    "            unacc_unacc += 1\n",
    "        elif actual[2] and predicted[3]:\n",
    "            unacc_vgood += 1\n",
    "        elif actual[3] and predicted[0]:\n",
    "            vgood_acc += 1\n",
    "        elif actual[3] and predicted[1]:\n",
    "            vgood_good += 1\n",
    "        elif actual[3] and predicted[2]:\n",
    "            vgood_unacc += 1\n",
    "        elif actual[3] and predicted[3]:\n",
    "            vgood_vgood += 1\n",
    "\n",
    "    # Confusion matrix as an array\n",
    "    confusion_matrix = [\n",
    "        [acc_acc, acc_good, acc_unacc, acc_vgood],\n",
    "        [good_acc, good_good, good_unacc, good_vgood],\n",
    "        [unacc_acc, unacc_good, unacc_unacc, unacc_vgood],\n",
    "        [vgood_acc, vgood_good, vgood_unacc, vgood_vgood]\n",
    "    ]\n",
    "\n",
    "    # Accuracy\n",
    "    total_correct = acc_acc + good_good + unacc_unacc + vgood_vgood\n",
    "    total_predictions = sum(sum(row) for row in confusion_matrix)\n",
    "    accuracy = total_correct / total_predictions\n",
    "\n",
    "    # Precision calculations\n",
    "    Precision_of_acc = acc_acc / (acc_acc + good_acc + unacc_acc + vgood_acc)\n",
    "    Precision_of_good = good_good / (acc_good + good_good + unacc_good + vgood_good)\n",
    "    Precision_of_unacc = unacc_unacc / (acc_unacc + good_unacc + unacc_unacc + vgood_unacc)\n",
    "    Precision_of_vgood = vgood_vgood / (acc_vgood + good_vgood + unacc_vgood + vgood_vgood)\n",
    "    average_precision = (Precision_of_acc + Precision_of_good + Precision_of_unacc + Precision_of_vgood) / 4.0\n",
    "\n",
    "    # Recall calculations\n",
    "    Recall_of_acc = acc_acc / (acc_acc + acc_good + acc_unacc + acc_vgood)\n",
    "    Recall_of_good = good_good / (good_acc + good_good + good_unacc + good_vgood)\n",
    "    Recall_of_unacc = unacc_unacc / (unacc_acc + unacc_good + unacc_unacc + unacc_vgood)\n",
    "    Recall_of_vgood = vgood_vgood / (vgood_acc + vgood_good + vgood_unacc + vgood_vgood)\n",
    "    average_recall = (Recall_of_acc + Recall_of_good + Recall_of_unacc + Recall_of_vgood) / 4.0\n",
    "\n",
    "    # F1 Score\n",
    "    average_f1_score = 2 * ((average_precision * average_recall) / (average_precision + average_recall))\n",
    "\n",
    "    # Print or return metrics\n",
    "    print(\"Confusion Matrix:\")\n",
    "    for row in confusion_matrix:\n",
    "        print(row)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Average Precision: {average_precision}\")\n",
    "    print(f\"Average Recall: {average_recall}\")\n",
    "    print(f\"Average F1 Score: {average_f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a10872db-6751-4c5a-9aab-8be58d58b297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results of Decision Tree: \n",
      "Confusion Matrix:\n",
      "[50, 2, 3, 2]\n",
      "[1, 8, 0, 1]\n",
      "[1, 0, 177, 0]\n",
      "[1, 1, 0, 12]\n",
      "Accuracy: 0.9536679536679536\n",
      "Average Precision: 0.8635005717552888\n",
      "Average Recall: 0.8821794655177269\n",
      "Average F1 Score: 0.8727400859269795\n",
      "-------------------------------\n",
      "Test results of Decision Tree: \n",
      "Confusion Matrix:\n",
      "[51, 3, 6, 1]\n",
      "[0, 9, 0, 0]\n",
      "[2, 0, 178, 0]\n",
      "[2, 1, 0, 7]\n",
      "Accuracy: 0.9423076923076923\n",
      "Average Precision: 0.8654929309820614\n",
      "Average Recall: 0.8812386156648453\n",
      "Average F1 Score: 0.8732948046085969\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_val = model.predict(X_val)\n",
    "print(\"Validation results of Decision Tree: \")\n",
    "calculate_metrics(y_val, y_pred_val)\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "y_pred_test = model.predict(X_test)\n",
    "print(\"Test results of Decision Tree: \")\n",
    "calculate_metrics(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fd65e00f-2556-4d40-a972-d074f9a20c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results for Model1: \n",
      "Confusion Matrix:\n",
      "[48, 2, 4, 2]\n",
      "[0, 8, 0, 1]\n",
      "[1, 0, 176, 0]\n",
      "[1, 2, 0, 11]\n",
      "Accuracy: 0.94921875\n",
      "Average Precision: 0.8475396825396825\n",
      "Average Recall: 0.8815240785579769\n",
      "Average F1 Score: 0.8641979023582292\n",
      "-------------------------------\n",
      "Validation results for Model2: \n",
      "Confusion Matrix:\n",
      "[46, 1, 4, 2]\n",
      "[0, 8, 0, 1]\n",
      "[3, 0, 175, 0]\n",
      "[1, 2, 0, 11]\n",
      "Accuracy: 0.9448818897637795\n",
      "Average Precision: 0.8526601610679823\n",
      "Average Recall: 0.8814184425801979\n",
      "Average F1 Score: 0.8668008354841572\n",
      "-------------------------------\n",
      "Validation results for Model3: \n",
      "Confusion Matrix:\n",
      "[47, 2, 7, 1]\n",
      "[2, 6, 0, 2]\n",
      "[3, 0, 175, 0]\n",
      "[5, 0, 0, 9]\n",
      "Accuracy: 0.915057915057915\n",
      "Average Precision: 0.8215249662618084\n",
      "Average Recall: 0.7626411534454113\n",
      "Average F1 Score: 0.7909886975362169\n",
      "-------------------------------\n",
      "Validation results for Model4: \n",
      "Confusion Matrix:\n",
      "[52, 1, 2, 2]\n",
      "[0, 8, 0, 1]\n",
      "[0, 0, 178, 0]\n",
      "[1, 1, 0, 12]\n",
      "Accuracy: 0.9689922480620154\n",
      "Average Precision: 0.8925052410901468\n",
      "Average Recall: 0.9145781119465329\n",
      "Average F1 Score: 0.9034068704433948\n",
      "-------------------------------\n",
      "Test results for Model4: \n",
      "Confusion Matrix:\n",
      "[56, 2, 1, 1]\n",
      "[0, 9, 0, 0]\n",
      "[2, 0, 178, 0]\n",
      "[2, 1, 0, 7]\n",
      "Accuracy: 0.9652509652509652\n",
      "Average Precision: 0.8881866852886406\n",
      "Average Recall: 0.9055555555555557\n",
      "Average F1 Score: 0.8967870287261233\n"
     ]
    }
   ],
   "source": [
    "model1 = DecisionTreeClassifier(max_depth=10, min_samples_split=4, criterion='gini', min_samples_leaf=1, random_state=42)\n",
    "model2 = DecisionTreeClassifier(max_depth=9, min_samples_split=5, criterion='entropy', min_samples_leaf=4, random_state=42)\n",
    "model3 = DecisionTreeClassifier(max_depth=8, min_samples_split=2, criterion='gini', min_samples_leaf=10, random_state=42)\n",
    "model4 = DecisionTreeClassifier(max_depth=11, min_samples_split=2, criterion='entropy', min_samples_leaf=1, random_state=42)\n",
    "\n",
    "model1.fit(X_train, y_train)\n",
    "model2.fit(X_train, y_train)\n",
    "model3.fit(X_train, y_train)\n",
    "model4.fit(X_train, y_train)\n",
    "\n",
    "y_pred_val1 = model1.predict(X_val)\n",
    "y_pred_val2 = model2.predict(X_val)\n",
    "y_pred_val3 = model3.predict(X_val)\n",
    "y_pred_val4 = model4.predict(X_val)\n",
    "\n",
    "# Validation results for Model1\n",
    "print(\"Validation results for Model1: \")\n",
    "calculate_metrics(y_val, y_pred_val1)\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "# Validation results for Model2\n",
    "print(\"Validation results for Model2: \")\n",
    "calculate_metrics(y_val, y_pred_val2)\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "# Validation results for Model3\n",
    "print(\"Validation results for Model3: \")\n",
    "calculate_metrics(y_val, y_pred_val3)\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "# Validation results for Model4\n",
    "print(\"Validation results for Model4: \")\n",
    "calculate_metrics(y_val, y_pred_val4)\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "# Test results for Model4\n",
    "y_pred_test = model4.predict(X_test)\n",
    "print(\"Test results for Model4: \")\n",
    "calculate_metrics(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ed023751-4544-4bbd-95d0-c6a768c70bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results of XGBoost: \n",
      "Confusion Matrix:\n",
      "[52, 2, 0, 1]\n",
      "[0, 7, 0, 1]\n",
      "[0, 0, 178, 0]\n",
      "[1, 1, 0, 11]\n",
      "Accuracy: 0.9763779527559056\n",
      "Average Precision: 0.8818214804063861\n",
      "Average Recall: 0.9166520979020979\n",
      "Average F1 Score: 0.8988995109396035\n",
      "-------------------------------\n",
      "Test results of XGBoost: \n",
      "Confusion Matrix:\n",
      "[52, 4, 0, 0]\n",
      "[0, 8, 0, 0]\n",
      "[1, 0, 179, 0]\n",
      "[0, 0, 0, 7]\n",
      "Accuracy: 0.9800796812749004\n",
      "Average Precision: 0.9119496855345912\n",
      "Average Recall: 0.9807539682539683\n",
      "Average F1 Score: 0.9451012271738607\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_val)\n",
    "print(\"Validation results of XGBoost: \")\n",
    "calculate_metrics(y_val, y_pred_xgb)\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "print(\"Test results of XGBoost: \")\n",
    "calculate_metrics(y_test, y_pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ba58731b-dfff-49ab-91e3-8800a6da0485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for Model 1:\n",
      "Confusion Matrix:\n",
      "[47, 2, 0, 1]\n",
      "[0, 3, 0, 1]\n",
      "[0, 0, 174, 0]\n",
      "[1, 0, 0, 11]\n",
      "Accuracy: 0.9791666666666666\n",
      "Average Precision: 0.8563301282051282\n",
      "Average Recall: 0.9016666666666666\n",
      "Average F1 Score: 0.8784138111255951\n",
      "-------------------------------\n",
      "\n",
      "Metrics for Model 2:\n",
      "Confusion Matrix:\n",
      "[51, 2, 0, 0]\n",
      "[1, 3, 0, 0]\n",
      "[0, 0, 177, 0]\n",
      "[1, 0, 0, 10]\n",
      "Accuracy: 0.9836734693877551\n",
      "Average Precision: 0.8905660377358491\n",
      "Average Recall: 0.9053387650085764\n",
      "Average F1 Score: 0.897891642731014\n",
      "-------------------------------\n",
      "\n",
      "Metrics for Model 3:\n",
      "Confusion Matrix:\n",
      "[53, 2, 0, 1]\n",
      "[1, 7, 0, 1]\n",
      "[0, 0, 178, 0]\n",
      "[1, 0, 0, 12]\n",
      "Accuracy: 0.9765625\n",
      "Average Precision: 0.8996392496392497\n",
      "Average Recall: 0.911820818070818\n",
      "Average F1 Score: 0.9056890750141248\n",
      "-------------------------------\n",
      "\n",
      "Metrics for Model 4:\n",
      "Confusion Matrix:\n",
      "[53, 2, 0, 1]\n",
      "[1, 7, 0, 1]\n",
      "[0, 0, 178, 0]\n",
      "[1, 0, 0, 12]\n",
      "Accuracy: 0.9765625\n",
      "Average Precision: 0.8996392496392497\n",
      "Average Recall: 0.911820818070818\n",
      "Average F1 Score: 0.9056890750141248\n",
      "-------------------------------\n",
      "Test results for Model4: \n",
      "Confusion Matrix:\n",
      "[54, 4, 0, 0]\n",
      "[0, 8, 0, 0]\n",
      "[1, 0, 179, 0]\n",
      "[1, 0, 0, 7]\n",
      "Accuracy: 0.9763779527559056\n",
      "Average Precision: 0.9077380952380952\n",
      "Average Recall: 0.9501197318007663\n",
      "Average F1 Score: 0.9284455064762251\n"
     ]
    }
   ],
   "source": [
    "model1 = XGBClassifier(max_depth=10, colsample_bytree=0.7, learning_rate=0.01, n_estimators=200,subsample=1.0)\n",
    "model2 = XGBClassifier(max_depth=11, colsample_bytree=0.8, learning_rate=0.05, n_estimators=100,subsample=0.5)\n",
    "model3 = XGBClassifier(max_depth=12, colsample_bytree=0.9, learning_rate=0.1, n_estimators=150,subsample=0.7)\n",
    "model4 = XGBClassifier(max_depth=13, colsample_bytree=1.0, learning_rate=0.2, n_estimators=170, subsample=0.9)\n",
    "\n",
    "model1.fit(X_train, y_train)\n",
    "model2.fit(X_train, y_train)\n",
    "model3.fit(X_train, y_train)\n",
    "model4.fit(X_train, y_train)\n",
    "\n",
    "y_pred_val1 = model1.predict(X_val)\n",
    "y_pred_val2 = model2.predict(X_val)\n",
    "y_pred_val3 = model3.predict(X_val)\n",
    "y_pred_val4 = model4.predict(X_val)\n",
    "\n",
    "# Validation results for model1\n",
    "print(\"\\nMetrics for Model 1:\")\n",
    "calculate_metrics(y_val, y_pred_val1)\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "# Validation results for model2\n",
    "print(\"\\nMetrics for Model 2:\")\n",
    "calculate_metrics(y_val, y_pred_val2)\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "# Validation results for model3\n",
    "print(\"\\nMetrics for Model 3:\")\n",
    "calculate_metrics(y_val, y_pred_val3)\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "# Validation results for model4\n",
    "print(\"\\nMetrics for Model 4:\")\n",
    "calculate_metrics(y_val, y_pred_val4)\n",
    "print(\"-------------------------------\")\n",
    "\n",
    "# Test results for Model4\n",
    "y_pred_test = model4.predict(X_test)\n",
    "print(\"Test results for Model4: \")\n",
    "calculate_metrics(y_test, y_pred_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
