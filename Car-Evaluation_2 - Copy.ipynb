{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "04b8dc55-935f-4dbe-a978-e7505c5d3407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd31d67-9fdc-4f9d-b113-d2091cf41500",
   "metadata": {},
   "source": [
    "__Importing the Car Evaluation Dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "63cdbea0-3c0f-43c0-9476-e78c31ff52a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "car_evaluation = fetch_ucirepo(id=19) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = car_evaluation.data.features \n",
    "y = car_evaluation.data.targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6270681a-0067-4fc4-b1fe-527dc65ef018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(1728, 6)\n",
      "(1728, 1)\n"
     ]
    }
   ],
   "source": [
    "print(type(X))\n",
    "print(type(y))\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccb4b1f-9e7b-4444-baaa-7f9e170833bd",
   "metadata": {},
   "source": [
    "__One Hot Encoding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "986e15d7-57c7-4db0-b08d-374c89539dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(1728, 21)\n",
      "(1728, 4)\n"
     ]
    }
   ],
   "source": [
    "X_encoded = pd.get_dummies(X) # One hot encoding\n",
    "y_encoded = pd.get_dummies(y) # One hot encoding\n",
    "\n",
    "# print(X_encoded)\n",
    "# print(y_encoded)\n",
    "\n",
    "print(type(X))\n",
    "print(type(y))\n",
    "\n",
    "print(X_encoded.shape)\n",
    "print(y_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098c3739-2b76-49a8-855d-dd581d7e92ca",
   "metadata": {},
   "source": [
    "__Dataset Partitioning__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "30a0b76c-11c1-42ec-b3e4-52452cc2a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# data split, 70% training and 30% temp (temp = validation + test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_encoded, y_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# 30% temp data into 15% validation and 15% test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ca26ff-3bb0-4065-8034-8ae886e8ed93",
   "metadata": {},
   "source": [
    "__Building a Single Decision Tree__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "394fe74e-e915-4cdc-add3-99ea460ed410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for Model 1:\n",
      "Confusion Matrix:\n",
      "[50, 2, 3, 2]\n",
      "[0, 9, 0, 1]\n",
      "[0, 0, 178, 0]\n",
      "[1, 1, 0, 12]\n",
      "Accuracy: 0.9613899613899614\n",
      "Average Precision: 0.8784543928068465\n",
      "Average Recall: 0.9085839598997494\n",
      "Average F1 Score: 0.8932651832557713\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    # Initialize confusion matrix counts\n",
    "    acc_acc = acc_good = acc_unacc = acc_vgood = 0\n",
    "    good_acc = good_good = good_unacc = good_vgood = 0\n",
    "    unacc_acc = unacc_good = unacc_unacc = unacc_vgood = 0\n",
    "    vgood_acc = vgood_good = vgood_unacc = vgood_vgood = 0\n",
    "\n",
    "    y_true = y_true.to_numpy()\n",
    "    \n",
    "    # # Calculate confusion matrix manually\n",
    "    # for actual, predicted in zip(y_true, y_pred):\n",
    "    for i in range(len(y_true)):\n",
    "        actual = y_true[i]\n",
    "        # actual = y_true.iloc[i].values # <class 'pandas.core.frame.DataFrame'>\n",
    "        predicted = y_pred[i] # <class 'numpy.ndarray'>\n",
    "        \n",
    "        if actual[0] and predicted[0]:\n",
    "            acc_acc += 1\n",
    "        elif actual[0] and predicted[1]:\n",
    "            acc_good += 1\n",
    "        elif actual[0] and predicted[2]:\n",
    "            acc_unacc += 1\n",
    "        elif actual[0] and predicted[3]:\n",
    "            acc_vgood += 1\n",
    "        elif actual[1] and predicted[0]:\n",
    "            good_acc += 1\n",
    "        elif actual[1] and predicted[1]:\n",
    "            good_good += 1\n",
    "        elif actual[1] and predicted[2]:\n",
    "            good_unacc += 1\n",
    "        elif actual[1] and predicted[3]:\n",
    "            good_vgood += 1\n",
    "        elif actual[2] and predicted[0]:\n",
    "            unacc_acc += 1\n",
    "        elif actual[2] and predicted[1]:\n",
    "            unacc_good += 1\n",
    "        elif actual[2] and predicted[2]:\n",
    "            unacc_unacc += 1\n",
    "        elif actual[2] and predicted[3]:\n",
    "            unacc_vgood += 1\n",
    "        elif actual[3] and predicted[0]:\n",
    "            vgood_acc += 1\n",
    "        elif actual[3] and predicted[1]:\n",
    "            vgood_good += 1\n",
    "        elif actual[3] and predicted[2]:\n",
    "            vgood_unacc += 1\n",
    "        elif actual[3] and predicted[3]:\n",
    "            vgood_vgood += 1\n",
    "\n",
    "    # Confusion matrix as an array\n",
    "    confusion_matrix = [\n",
    "        [acc_acc, acc_good, acc_unacc, acc_vgood],\n",
    "        [good_acc, good_good, good_unacc, good_vgood],\n",
    "        [unacc_acc, unacc_good, unacc_unacc, unacc_vgood],\n",
    "        [vgood_acc, vgood_good, vgood_unacc, vgood_vgood]\n",
    "    ]\n",
    "\n",
    "    # Accuracy\n",
    "    total_correct = acc_acc + good_good + unacc_unacc + vgood_vgood\n",
    "    total_predictions = sum(sum(row) for row in confusion_matrix)\n",
    "    accuracy = total_correct / total_predictions\n",
    "\n",
    "    # Precision calculations\n",
    "    Precision_of_acc = acc_acc / (acc_acc + good_acc + unacc_acc + vgood_acc)\n",
    "    Precision_of_good = good_good / (acc_good + good_good + unacc_good + vgood_good)\n",
    "    Precision_of_unacc = unacc_unacc / (acc_unacc + good_unacc + unacc_unacc + vgood_unacc)\n",
    "    Precision_of_vgood = vgood_vgood / (acc_vgood + good_vgood + unacc_vgood + vgood_vgood)\n",
    "    average_precision = (Precision_of_acc + Precision_of_good + Precision_of_unacc + Precision_of_vgood) / 4.0\n",
    "\n",
    "    # Recall calculations\n",
    "    Recall_of_acc = acc_acc / (acc_acc + acc_good + acc_unacc + acc_vgood)\n",
    "    Recall_of_good = good_good / (good_acc + good_good + good_unacc + good_vgood)\n",
    "    Recall_of_unacc = unacc_unacc / (unacc_acc + unacc_good + unacc_unacc + unacc_vgood)\n",
    "    Recall_of_vgood = vgood_vgood / (vgood_acc + vgood_good + vgood_unacc + vgood_vgood)\n",
    "    average_recall = (Recall_of_acc + Recall_of_good + Recall_of_unacc + Recall_of_vgood) / 4.0\n",
    "\n",
    "    # F1 Score\n",
    "    average_f1_score = 2 * ((average_precision * average_recall) / (average_precision + average_recall))\n",
    "\n",
    "    # Print or return metrics\n",
    "    print(\"Confusion Matrix:\")\n",
    "    for row in confusion_matrix:\n",
    "        print(row)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Average Precision: {average_precision}\")\n",
    "    print(f\"Average Recall: {average_recall}\")\n",
    "    print(f\"Average F1 Score: {average_f1_score}\")\n",
    "\n",
    "    # return {\n",
    "    #     'accuracy': accuracy,\n",
    "    #     'average_precision': average_precision,\n",
    "    #     'average_recall': average_recall,\n",
    "    #     'average_f1_score': average_f1_score,\n",
    "    #     'confusion_matrix': confusion_matrix\n",
    "    # }\n",
    "\n",
    "model1 = DecisionTreeClassifier()\n",
    "model1.fit(X_train, y_train)\n",
    "y_pred1 = model1.predict(X_val)\n",
    "print(\"\\nMetrics for Model 1:\")\n",
    "calculate_metrics(y_val, y_pred1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "35b04153-a5e1-4700-a45c-e4942cb70784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for Model 1:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[179], line 112\u001b[0m\n\u001b[0;32m    110\u001b[0m y_pred1 \u001b[38;5;241m=\u001b[39m model1\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMetrics for Model 1:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 112\u001b[0m calculate_metrics(y_val, y_pred1)\n",
      "Cell \u001b[1;32mIn[179], line 21\u001b[0m, in \u001b[0;36mcalculate_metrics\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     19\u001b[0m     predicted \u001b[38;5;241m=\u001b[39m y_pred[i] \u001b[38;5;66;03m# <class 'numpy.ndarray'>\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# for actual, predicted in zip(y_true, y_pred):\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m actual \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m predicted \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     22\u001b[0m         acc_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m actual \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m predicted \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# for i in range(rows):\n",
    "#     actual = y_val.iloc[i].values # <class 'pandas.core.frame.DataFrame'>\n",
    "#     predicted = y_val_pred[i] # <class 'numpy.ndarray'>\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    # Initialize confusion matrix counts\n",
    "    acc_acc = acc_good = acc_unacc = acc_vgood = 0\n",
    "    good_acc = good_good = good_unacc = good_vgood = 0\n",
    "    unacc_acc = unacc_good = unacc_unacc = unacc_vgood = 0\n",
    "    vgood_acc = vgood_good = vgood_unacc = vgood_vgood = 0\n",
    "\n",
    "    # Calculate confusion matrix manually\n",
    "    for i in range(rows):\n",
    "        actual = y_true.iloc[i].values # <class 'pandas.core.frame.DataFrame'>\n",
    "        predicted = y_pred[i] # <class 'numpy.ndarray'>\n",
    "    # for actual, predicted in zip(y_true, y_pred):\n",
    "        if actual == 0 and predicted == 0:\n",
    "            acc_acc += 1\n",
    "        elif actual == 0 and predicted == 1:\n",
    "            acc_good += 1\n",
    "        elif actual == 0 and predicted == 2:\n",
    "            acc_unacc += 1\n",
    "        elif actual == 0 and predicted == 3:\n",
    "            acc_vgood += 1\n",
    "        elif actual == 1 and predicted == 0:\n",
    "            good_acc += 1\n",
    "        elif actual == 1 and predicted == 1:\n",
    "            good_good += 1\n",
    "        elif actual == 1 and predicted == 2:\n",
    "            good_unacc += 1\n",
    "        elif actual == 1 and predicted == 3:\n",
    "            good_vgood += 1\n",
    "        elif actual == 2 and predicted == 0:\n",
    "            unacc_acc += 1\n",
    "        elif actual == 2 and predicted == 1:\n",
    "            unacc_good += 1\n",
    "        elif actual == 2 and predicted == 2:\n",
    "            unacc_unacc += 1\n",
    "        elif actual == 2 and predicted == 3:\n",
    "            unacc_vgood += 1\n",
    "        elif actual == 3 and predicted == 0:\n",
    "            vgood_acc += 1\n",
    "        elif actual == 3 and predicted == 1:\n",
    "            vgood_good += 1\n",
    "        elif actual == 3 and predicted == 2:\n",
    "            vgood_unacc += 1\n",
    "        elif actual == 3 and predicted == 3:\n",
    "            vgood_vgood += 1\n",
    "\n",
    "    # Confusion matrix as an array\n",
    "    confusion_matrix = [\n",
    "        [acc_acc, acc_good, acc_unacc, acc_vgood],\n",
    "        [good_acc, good_good, good_unacc, good_vgood],\n",
    "        [unacc_acc, unacc_good, unacc_unacc, unacc_vgood],\n",
    "        [vgood_acc, vgood_good, vgood_unacc, vgood_vgood]\n",
    "    ]\n",
    "\n",
    "    # Accuracy\n",
    "    total_correct = acc_acc + good_good + unacc_unacc + vgood_vgood\n",
    "    total_predictions = sum(sum(row) for row in confusion_matrix)\n",
    "    accuracy = total_correct / total_predictions if total_predictions > 0 else 0\n",
    "\n",
    "    # Precision calculations (with manual check for division by zero)\n",
    "    Precision_of_acc = acc_acc / (acc_acc + good_acc + unacc_acc + vgood_acc) if (acc_acc + good_acc + unacc_acc + vgood_acc) != 0 else 0\n",
    "    Precision_of_good = good_good / (acc_good + good_good + unacc_good + vgood_good) if (acc_good + good_good + unacc_good + vgood_good) != 0 else 0\n",
    "    Precision_of_unacc = unacc_unacc / (acc_unacc + good_unacc + unacc_unacc + vgood_unacc) if (acc_unacc + good_unacc + unacc_unacc + vgood_unacc) != 0 else 0\n",
    "    Precision_of_vgood = vgood_vgood / (acc_vgood + good_vgood + unacc_vgood + vgood_vgood) if (acc_vgood + good_vgood + unacc_vgood + vgood_vgood) != 0 else 0\n",
    "\n",
    "    # Average precision\n",
    "    average_precision = (Precision_of_acc + Precision_of_good + Precision_of_unacc + Precision_of_vgood) / 4.0\n",
    "\n",
    "    # Recall calculations (with manual check for division by zero)\n",
    "    Recall_of_acc = acc_acc / (acc_acc + acc_good + acc_unacc + acc_vgood) if (acc_acc + acc_good + acc_unacc + acc_vgood) != 0 else 0\n",
    "    Recall_of_good = good_good / (good_acc + good_good + good_unacc + good_vgood) if (good_acc + good_good + good_unacc + good_vgood) != 0 else 0\n",
    "    Recall_of_unacc = unacc_unacc / (unacc_acc + unacc_good + unacc_unacc + unacc_vgood) if (unacc_acc + unacc_good + unacc_unacc + unacc_vgood) != 0 else 0\n",
    "    Recall_of_vgood = vgood_vgood / (vgood_acc + vgood_good + vgood_unacc + vgood_vgood) if (vgood_acc + vgood_good + vgood_unacc + vgood_vgood) != 0 else 0\n",
    "\n",
    "    # Average recall\n",
    "    average_recall = (Recall_of_acc + Recall_of_good + Recall_of_unacc + Recall_of_vgood) / 4.0\n",
    "\n",
    "    # F1 Score calculation (with manual check for division by zero)\n",
    "    average_f1_score = 2 * ((average_precision * average_recall) / (average_precision + average_recall)) if (average_precision + average_recall) != 0 else 0\n",
    "\n",
    "    # Print or return metrics\n",
    "    print(\"Confusion Matrix:\")\n",
    "    for row in confusion_matrix:\n",
    "        print(row)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Average Precision: {average_precision}\")\n",
    "    print(f\"Average Recall: {average_recall}\")\n",
    "    print(f\"Average F1 Score: {average_f1_score}\")\n",
    "\n",
    "    # return {\n",
    "    #     'accuracy': accuracy,\n",
    "    #     'average_precision': average_precision,\n",
    "    #     'average_recall': average_recall,\n",
    "    #     'average_f1_score': average_f1_score,\n",
    "    #     'confusion_matrix': confusion_matrix\n",
    "    # }\n",
    "\n",
    "\n",
    "\n",
    "model1 = DecisionTreeClassifier()\n",
    "model1.fit(X_train, y_train)\n",
    "y_pred1 = model1.predict(X_val)\n",
    "print(\"\\nMetrics for Model 1:\")\n",
    "calculate_metrics(y_val, y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcca6dab-09d4-49c5-8363-6d3ab749e299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# # Define models\n",
    "# model1 = DecisionTreeClassifier(max_depth=10, min_samples_split=4, criterion='gini', min_samples_leaf=1, random_state=42)\n",
    "# model2 = DecisionTreeClassifier(max_depth=9, min_samples_split=5, criterion='entropy', min_samples_leaf=4, random_state=42)\n",
    "# model3 = DecisionTreeClassifier(max_depth=5, min_samples_split=2, criterion='gini', min_samples_leaf=10, random_state=42)\n",
    "# model4 = DecisionTreeClassifier(max_depth=11, min_samples_split=2, criterion='entropy', min_samples_leaf=1, random_state=42)\n",
    "model1 = DecisionTreeClassifier()\n",
    "model1.fit(X_train, y_train)\n",
    "y_pred1 = model1.predict(X_val)\n",
    "print(\"\\nMetrics for Model 1:\")\n",
    "calculate_metrics(y_val, y_pred1)\n",
    "\n",
    "# # Train and evaluate model 1\n",
    "# model1.fit(X_train, y_train)\n",
    "# y_pred1 = model1.predict(X_val)\n",
    "# print(\"\\nMetrics for Model 1:\")\n",
    "# calculate_metrics(y_val, y_pred1)\n",
    "\n",
    "# # Train and evaluate model 2\n",
    "# model2.fit(X_train, y_train)\n",
    "# y_pred2 = model2.predict(X_val)\n",
    "# print(\"\\nMetrics for Model 2:\")\n",
    "# calculate_metrics(y_val, y_pred2)\n",
    "\n",
    "# # Train and evaluate model 3\n",
    "# model3.fit(X_train, y_train)\n",
    "# y_pred3 = model3.predict(X_val)\n",
    "# print(\"\\nMetrics for Model 3:\")\n",
    "# calculate_metrics(y_val, y_pred3)\n",
    "\n",
    "# # Train and evaluate model 4\n",
    "# model4.fit(X_train, y_train)\n",
    "# y_pred4 = model4.predict(X_val)\n",
    "# print(\"\\nMetrics for Model 4:\")\n",
    "# calculate_metrics(y_val, y_pred4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c933ef-8bf6-409b-b408-0282138bb29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "single_decision_tree_classifier = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f59881-5604-4c5a-bd8a-fe178134d182",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_decision_tree_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d97061-40ee-4faf-ad8c-0dd1e83eab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import plot_tree\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(40, 20))\n",
    "# plot_tree(single_decision_tree_classifier, filled=True, feature_names=X_train.columns, class_names=single_decision_tree_classifier.classes_)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535c1f37-d56b-4dc4-b02f-0cc5baa5c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# dt_params = {'max_depth': [3,4,5,6,7,8,9,10,11], 'min_samples_split': [2,3,4,5,6,7,8,9,10,11],'min_samples_leaf':[1,2,3,4,5],'random_state':[42]}\n",
    "# dt_grid = GridSearchCV(DecisionTreeClassifier(), dt_params, cv=5, scoring='accuracy')\n",
    "# dt_grid.fit(X_train, y_train)\n",
    "# best_params = dt_grid.best_params_\n",
    "# print(f\"Best Hyperparameters:Â {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4a4b5c-7139-49c7-94a3-6743fc3f5adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "model1 = DecisionTreeClassifier(max_depth=10, min_samples_split=4, criterion='gini', min_samples_leaf=1, random_state=42)\n",
    "model2 = DecisionTreeClassifier(max_depth=9, min_samples_split=5, criterion='entropy', min_samples_leaf=4, random_state=42)\n",
    "model3 = DecisionTreeClassifier(max_depth=5, min_samples_split=2, criterion='gini', min_samples_leaf=10, random_state=42)\n",
    "model4 = DecisionTreeClassifier(max_depth=11, min_samples_split=2, criterion='entropy', min_samples_leaf=1, random_state=42)\n",
    "\n",
    "\n",
    "# Fit models\n",
    "model1.fit(X_train, y_train)\n",
    "model2.fit(X_train, y_train)\n",
    "model3.fit(X_train, y_train)\n",
    "model4.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predictions on validation set\n",
    "y_pred_val1 = model1.predict(X_val)\n",
    "y_pred_val2 = model2.predict(X_val)\n",
    "y_pred_val3 = model3.predict(X_val)\n",
    "y_pred_val4 = model4.predict(X_val)\n",
    "\n",
    "\n",
    "# Validation results for model1\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val1)\n",
    "val_precision, val_recall, val_f1, val_support = precision_recall_fscore_support(\n",
    "    y_val, y_pred_val1, average='macro', zero_division=1\n",
    ")\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Validation results for Model 1: \")\n",
    "print(f\"Accuracy: {val_accuracy}\")\n",
    "print(f\"Average Precision: {val_precision}\")\n",
    "print(f\"Average Recall: {val_recall}\")\n",
    "print(f\"Average F1-score: {val_f1}\")\n",
    "\n",
    "\n",
    "# Validation results for model2\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val2)\n",
    "val_precision, val_recall, val_f1, val_support = precision_recall_fscore_support(\n",
    "    y_val, y_pred_val2, average='macro', zero_division=1\n",
    ")\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Validation results for Model 2: \")\n",
    "print(f\"Accuracy: {val_accuracy}\")\n",
    "print(f\"Average Precision: {val_precision}\")\n",
    "print(f\"Average Recall: {val_recall}\")\n",
    "print(f\"Average F1-score: {val_f1}\")\n",
    "\n",
    "\n",
    "# Validation results for model3\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val3)\n",
    "val_precision, val_recall, val_f1, val_support = precision_recall_fscore_support(\n",
    "    y_val, y_pred_val3, average='macro', zero_division=1\n",
    ")\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Validation results for Model 3: \")\n",
    "print(f\"Accuracy: {val_accuracy}\")\n",
    "print(f\"Average Precision: {val_precision}\")\n",
    "print(f\"Average Recall: {val_recall}\")\n",
    "print(f\"Average F1-score: {val_f1}\")\n",
    "\n",
    "\n",
    "# Validation results for model4\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val4)\n",
    "val_precision, val_recall, val_f1, val_support = precision_recall_fscore_support(\n",
    "    y_val, y_pred_val4, average='macro', zero_division=1\n",
    ")\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Validation results for Model 4: \")\n",
    "print(f\"Accuracy: {val_accuracy}\")\n",
    "print(f\"Average Precision: {val_precision}\")\n",
    "print(f\"Average Recall: {val_recall}\")\n",
    "print(f\"Average F1-score: {val_f1}\")\n",
    "\n",
    "\n",
    "# Test set results for model4\n",
    "y_pred_test = model4.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "test_precision, test_recall, test_f1, test_support = precision_recall_fscore_support(\n",
    "    y_test, y_pred_test, average='macro', zero_division=1\n",
    ")\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Test Sets Results using Model 4: \")\n",
    "print(f\"Accuracy: {test_accuracy}\")\n",
    "print(f\"Average Precision: {test_precision}\")\n",
    "print(f\"Average Recall: {test_recall}\")\n",
    "print(f\"Average F1-score: {test_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f09a44-7017-4aba-a862-00f286ac4916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Hyperparameter Tuning for Decision Tree Classifier\n",
    "# dt_params = {'max_depth': [10, 11, 12, 13, 14, 15, 16], 'min_samples_split': [2, 3, 4, 5, 5, 6, 7, 8, 9]}\n",
    "# dt_grid = GridSearchCV(DecisionTreeClassifier(), dt_params, cv=5, scoring='accuracy')\n",
    "# dt_grid.fit(X_train, y_train)\n",
    "\n",
    "# best_params = dt_grid.best_params_\n",
    "# print(f\"Best Hyperparameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9718f7-4a18-4070-a90a-3c47077c2fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Retrieve the best parameters from GridSearchCV\n",
    "# # best_params = dt_grid.best_params_\n",
    "\n",
    "# # Step 2: Initialize a new Decision Tree Classifier with the best parameters\n",
    "# best_dt_model = DecisionTreeClassifier(**best_params)\n",
    "\n",
    "# # Step 3: Fit the model on the training set\n",
    "# best_dt_model.fit(X_train, y_train)\n",
    "\n",
    "# # Step 4: Evaluate on the validation set\n",
    "# y_val_pred = best_dt_model.predict(X_val)\n",
    "# val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "# print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# # Step 5: Evaluate on the test set\n",
    "# y_test_pred = best_dt_model.predict(X_test)\n",
    "# test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "# print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad93a20-8393-4cbd-b685-c88c768a7ea5",
   "metadata": {},
   "source": [
    "__Validation Set__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96147f39-d7eb-4763-af53-de2c92563f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_val contains actual ground truth labels of the validation set\n",
    "# y_val_pred is our model's prediction on the X_val (validation set)\n",
    "\n",
    "y_val_pred = single_decision_tree_classifier.predict(X_val)\n",
    "\n",
    "print(y_val)\n",
    "print(type(y_val))\n",
    "print(y_val.shape)\n",
    "\n",
    "print(y_val_pred)\n",
    "print(type(y_val_pred))\n",
    "print(y_val_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b61f94-0774-49a2-ae7b-76c3fde70637",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_rows, y_val_cols = y_val.shape\n",
    "print(y_val_rows)\n",
    "print(y_val_cols)\n",
    "\n",
    "y_val_pred_rows, y_val_pred_cols = y_val_pred.shape\n",
    "print(y_val_pred_rows)\n",
    "print(y_val_pred_cols)\n",
    "\n",
    "if y_val_rows == y_val_pred_rows:\n",
    "    rows = y_val_rows\n",
    "    print(f\"rows = {rows}\")\n",
    "\n",
    "if y_val_cols == y_val_pred_cols:\n",
    "    cols = y_val_cols\n",
    "    print(f\"columns = {cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3b9f09-5589-4cc5-961a-1f7b6a7efe11",
   "metadata": {},
   "source": [
    "__Manually building the Confusion Matrix on the Validation Set__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919103a4-a4cf-4084-9128-407b0e1e8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_acc = acc_good = acc_unacc = acc_vgood = 0\n",
    "good_acc = good_good = good_unacc = good_vgood = 0\n",
    "unacc_acc = unacc_good = unacc_unacc = unacc_vgood = 0\n",
    "vgood_acc = vgood_good = vgood_unacc = vgood_vgood = 0\n",
    "\n",
    "for i in range(rows):\n",
    "    actual = y_val.iloc[i].values # <class 'pandas.core.frame.DataFrame'>\n",
    "    predicted = y_val_pred[i] # <class 'numpy.ndarray'>\n",
    "    \n",
    "    if actual[0] == True and predicted[0] == True:\n",
    "        acc_acc += 1\n",
    "    elif actual[0] == True and predicted[1] == True:\n",
    "        acc_good += 1\n",
    "    elif actual[0] == True and predicted[2] == True:\n",
    "        acc_unacc += 1\n",
    "    elif actual[0] == True and predicted[3] == True:\n",
    "        acc_vgood += 1\n",
    "    \n",
    "    if actual[1] == True and predicted[0] == True:\n",
    "        good_acc += 1\n",
    "    elif actual[1] == True and predicted[1] == True:\n",
    "        good_good += 1\n",
    "    elif actual[1] == True and predicted[2] == True:\n",
    "        good_unacc += 1\n",
    "    elif actual[1] == True and predicted[3] == True:\n",
    "        good_vgood += 1\n",
    "\n",
    "    if actual[2] == True and predicted[0] == True:\n",
    "        unacc_acc += 1\n",
    "    elif actual[2] == True and predicted[1] == True:\n",
    "        unacc_good += 1\n",
    "    elif actual[2] == True and predicted[2] == True:\n",
    "        unacc_unacc += 1\n",
    "    elif actual[2] == True and predicted[3] == True:\n",
    "        unacc_vgood += 1\n",
    "\n",
    "    if actual[3] == True and predicted[0] == True:\n",
    "        vgood_acc += 1\n",
    "    elif actual[3] == True and predicted[1] == True:\n",
    "        vgood_good += 1\n",
    "    elif actual[3] == True and predicted[2] == True:\n",
    "        vgood_unacc += 1\n",
    "    elif actual[3] == True and predicted[3] == True:\n",
    "        vgood_vgood += 1\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(f\"[{acc_acc}, {acc_good}, {acc_unacc}, {acc_vgood}]\")\n",
    "print(f\"[{good_acc}, {good_good}, {good_unacc}, {good_vgood}]\")\n",
    "print(f\"[{unacc_acc}, {unacc_good}, {unacc_unacc}, {unacc_vgood}]\")\n",
    "print(f\"[{vgood_acc}, {vgood_good}, {vgood_unacc}, {vgood_vgood}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bff66b8-ad22-4fe1-a3ba-de9c93968135",
   "metadata": {},
   "source": [
    "__Creating a List Containing the Confusion Matrix of Validation Set__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38235966-9cf5-4997-8de7-fdb4c3119137",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = [[acc_acc, acc_good, acc_unacc, acc_vgood],\n",
    "                   [good_acc, good_good, good_unacc, good_vgood],\n",
    "                   [unacc_acc, unacc_good, unacc_unacc, unacc_vgood],\n",
    "                   [vgood_acc, vgood_good, vgood_unacc, vgood_vgood]]\n",
    "print(confusion_matrix)\n",
    "print(type(confusion_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c062d699-c37e-481b-88c5-0ffb3c0c6c1b",
   "metadata": {},
   "source": [
    "__Manually Calculating the Validation Set's Accuracy from the Manually Built Confusion Matrix__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee731d5-39e9-4dda-8184-571fe78f3f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((acc_acc + good_good + unacc_unacc + vgood_vgood) / sum(sum(row) for row in confusion_matrix))\n",
    "\n",
    "# print((acc_acc + good_good + unacc_unacc + vgood_vgood) / (acc_acc + acc_good + acc_unacc + acc_vgood + good_acc + good_good + good_unacc + good_vgood + unacc_acc + unacc_good + unacc_unacc + unacc_vgood + vgood_acc + vgood_good + vgood_unacc + vgood_vgood))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5097a488-5299-4572-a293-0a6832520465",
   "metadata": {},
   "source": [
    "__Accuracy of the Single Decision Tree on the Validation Set__ `using the Sklearn Implementation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53977d44-243d-4971-9320-b955e607a6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# y_val_pred = single_decision_tree_classifier.predict(X_val)\n",
    "\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644e32d9-9936-4933-923d-80f831a4e54a",
   "metadata": {},
   "source": [
    "__Manually Calculating the Validation Set's Precision & Recall from the Manually Built Confusion Matrix__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9047a8-7d37-4fb6-9798-cd8a0f174d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision of acc\n",
    "# print(acc_acc / (acc_acc + good_acc + unacc_acc + vgood_acc))\n",
    "Precision_of_acc = acc_acc / (acc_acc + good_acc + unacc_acc + vgood_acc)\n",
    "print(f\"Precision of acc: {Precision_of_acc}\")\n",
    "\n",
    "# Precision of good\n",
    "# print(good_good / (acc_good + good_good + unacc_good + vgood_good))\n",
    "Precision_of_good = good_good / (acc_good + good_good + unacc_good + vgood_good)\n",
    "print(f\"Precision of good: {Precision_of_good}\")\n",
    "\n",
    "# Precision of unacc\n",
    "# print(unacc_unacc / (acc_unacc + good_unacc + unacc_unacc + vgood_unacc))\n",
    "Precision_of_unacc = unacc_unacc / (acc_unacc + good_unacc + unacc_unacc + vgood_unacc)\n",
    "print(f\"Precision of unacc: {Precision_of_unacc}\")\n",
    "\n",
    "# Precision of vgood\n",
    "# print(vgood_vgood / (acc_vgood + good_vgood + unacc_vgood + vgood_vgood))\n",
    "Precision_of_vgood = vgood_vgood / (acc_vgood + good_vgood + unacc_vgood + vgood_vgood)\n",
    "print(f\"Precision of vgood: {Precision_of_vgood}\")\n",
    "\n",
    "# Average precision\n",
    "average_precision = (Precision_of_acc + Precision_of_good + Precision_of_unacc + Precision_of_vgood) / 4.0\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"Average Precision: {average_precision}\")\n",
    "print(\"---------------------------------------\")\n",
    "\n",
    "# Recall of acc\n",
    "Recall_of_acc = acc_acc / (acc_acc + acc_good + acc_unacc + acc_vgood)\n",
    "print(f\"Recall of acc: {Recall_of_acc}\")\n",
    "\n",
    "# Recall of good\n",
    "Recall_of_good = good_good / (good_acc + good_good + good_unacc + good_vgood)\n",
    "print(f\"Recall of good: {Recall_of_good}\")\n",
    "\n",
    "# Recall of unacc\n",
    "Recall_of_unacc = unacc_unacc / (unacc_acc + unacc_good + unacc_unacc + unacc_vgood)\n",
    "print(f\"Recall of unacc: {Recall_of_unacc}\")\n",
    "\n",
    "# Recall of vgood\n",
    "Recall_of_vgood = vgood_vgood / (vgood_acc + vgood_good + vgood_unacc + vgood_vgood)\n",
    "print(f\"Recall of vgood: {Recall_of_vgood}\")\n",
    "\n",
    "# Average recall\n",
    "average_recall = (Recall_of_acc + Recall_of_good + Recall_of_unacc + Recall_of_vgood) / 4.0\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"Average recall: {average_recall}\")\n",
    "print(\"---------------------------------------\")\n",
    "\n",
    "# Average F1 Score\n",
    "average_f1_score = 2 * ((average_precision * average_recall) / (average_precision + average_recall))\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"Average F1 Score: {average_f1_score}\")\n",
    "print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3582357e-ad6c-4a7b-83a1-63c026fd94a3",
   "metadata": {},
   "source": [
    "__Confusion Matrix, Precision, Recall, F1 Score on the Validation Set__ `using the Sklearn Implementation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a895eb-6650-41f1-9665-d811d989d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "y_val_labels = np.argmax(y_val.values, axis=1)\n",
    "y_val_pred_labels = np.argmax(y_val_pred, axis=1)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_val_labels, y_val_pred_labels)\n",
    "\n",
    "print(\"Confusion Matrix using sklearn:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val_labels, y_val_pred_labels, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3f9e14-77fb-4f48-a473-3dd29832fc8c",
   "metadata": {},
   "source": [
    "__Test Set__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8d4b75-34e0-4dc7-993c-79d1daaf0547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test contains actual ground truth labels of the test set\n",
    "# y_test_pred is our model's prediction on the X_test (Test set)\n",
    "\n",
    "y_test_pred = single_decision_tree_classifier.predict(X_test)\n",
    "\n",
    "print(y_test)\n",
    "print(type(y_test))\n",
    "print(y_test.shape)\n",
    "\n",
    "print(y_test_pred)\n",
    "print(type(y_test_pred))\n",
    "print(y_test_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ba6c8a-c87b-4170-bd7a-6362e9f581ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_rows, y_test_cols = y_test.shape\n",
    "print(y_test_rows)\n",
    "print(y_test_cols)\n",
    "\n",
    "y_test_pred_rows, y_test_pred_cols = y_test_pred.shape\n",
    "print(y_test_pred_rows)\n",
    "print(y_test_pred_cols)\n",
    "\n",
    "if y_test_rows == y_test_pred_rows:\n",
    "    rows = y_test_rows\n",
    "    print(f\"rows = {rows}\")\n",
    "\n",
    "if y_test_cols == y_test_pred_cols:\n",
    "    cols = y_test_cols\n",
    "    print(f\"columns = {cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0fa8a9-14ca-4d53-9316-c1a18866f887",
   "metadata": {},
   "source": [
    "__Manually building the Confusion Matrix on the Test Set__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0e47d4-8328-45f2-bb52-ca8529e0ec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_acc = acc_good = acc_unacc = acc_vgood = 0\n",
    "good_acc = good_good = good_unacc = good_vgood = 0\n",
    "unacc_acc = unacc_good = unacc_unacc = unacc_vgood = 0\n",
    "vgood_acc = vgood_good = vgood_unacc = vgood_vgood = 0\n",
    "\n",
    "for i in range(rows):\n",
    "    actual = y_test.iloc[i].values # <class 'pandas.core.frame.DataFrame'>\n",
    "    predicted = y_test_pred[i] # <class 'numpy.ndarray'>\n",
    "    \n",
    "    if actual[0] == True and predicted[0] == True:\n",
    "        acc_acc += 1\n",
    "    elif actual[0] == True and predicted[1] == True:\n",
    "        acc_good += 1\n",
    "    elif actual[0] == True and predicted[2] == True:\n",
    "        acc_unacc += 1\n",
    "    elif actual[0] == True and predicted[3] == True:\n",
    "        acc_vgood += 1\n",
    "    \n",
    "    if actual[1] == True and predicted[0] == True:\n",
    "        good_acc += 1\n",
    "    elif actual[1] == True and predicted[1] == True:\n",
    "        good_good += 1\n",
    "    elif actual[1] == True and predicted[2] == True:\n",
    "        good_unacc += 1\n",
    "    elif actual[1] == True and predicted[3] == True:\n",
    "        good_vgood += 1\n",
    "\n",
    "    if actual[2] == True and predicted[0] == True:\n",
    "        unacc_acc += 1\n",
    "    elif actual[2] == True and predicted[1] == True:\n",
    "        unacc_good += 1\n",
    "    elif actual[2] == True and predicted[2] == True:\n",
    "        unacc_unacc += 1\n",
    "    elif actual[2] == True and predicted[3] == True:\n",
    "        unacc_vgood += 1\n",
    "\n",
    "    if actual[3] == True and predicted[0] == True:\n",
    "        vgood_acc += 1\n",
    "    elif actual[3] == True and predicted[1] == True:\n",
    "        vgood_good += 1\n",
    "    elif actual[3] == True and predicted[2] == True:\n",
    "        vgood_unacc += 1\n",
    "    elif actual[3] == True and predicted[3] == True:\n",
    "        vgood_vgood += 1\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(f\"[{acc_acc}, {acc_good}, {acc_unacc}, {acc_vgood}]\")\n",
    "print(f\"[{good_acc}, {good_good}, {good_unacc}, {good_vgood}]\")\n",
    "print(f\"[{unacc_acc}, {unacc_good}, {unacc_unacc}, {unacc_vgood}]\")\n",
    "print(f\"[{vgood_acc}, {vgood_good}, {vgood_unacc}, {vgood_vgood}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f565b87-83c2-4cac-987e-123d743874fa",
   "metadata": {},
   "source": [
    "__Creating a List Containing the Confusion Matrix of Test Set__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f035141-ef08-492c-95cb-485a323f0296",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = [[acc_acc, acc_good, acc_unacc, acc_vgood],\n",
    "                   [good_acc, good_good, good_unacc, good_vgood],\n",
    "                   [unacc_acc, unacc_good, unacc_unacc, unacc_vgood],\n",
    "                   [vgood_acc, vgood_good, vgood_unacc, vgood_vgood]]\n",
    "print(confusion_matrix)\n",
    "print(type(confusion_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a01a284-7401-48c1-a36b-adba5c833043",
   "metadata": {},
   "source": [
    "__Manually Calculating the Test Set's Accuracy from the Manually Built Confusion Matrix__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8e1d65-bdf8-4e96-b520-a44c16f16163",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((acc_acc + good_good + unacc_unacc + vgood_vgood) / sum(sum(row) for row in confusion_matrix))\n",
    "\n",
    "# print((acc_acc + good_good + unacc_unacc + vgood_vgood) / (acc_acc + acc_good + acc_unacc + acc_vgood + good_acc + good_good + good_unacc + good_vgood + unacc_acc + unacc_good + unacc_unacc + unacc_vgood + vgood_acc + vgood_good + vgood_unacc + vgood_vgood))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a6f41e-47b1-4fe3-a4e3-6f56bfb3a2e6",
   "metadata": {},
   "source": [
    "__Accuracy of the Single Decision Tree on the Test Set__ `using the Sklearn Implementation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3e78db-8bbc-4120-8164-cdce03598e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# y_test_pred = single_decision_tree_classifier.predict(X_test)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0b8dec-7736-447f-83bb-26ad89ff0fb9",
   "metadata": {},
   "source": [
    "__Manually Calculating the Test Set's Precision & Recall from the Manually Built Confusion Matrix__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1b2fb0-48ba-400f-9bb9-5960137d7957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision of acc\n",
    "# print(acc_acc / (acc_acc + good_acc + unacc_acc + vgood_acc))\n",
    "Precision_of_acc = acc_acc / (acc_acc + good_acc + unacc_acc + vgood_acc)\n",
    "print(f\"Precision of acc: {Precision_of_acc}\")\n",
    "\n",
    "# Precision of good\n",
    "# print(good_good / (acc_good + good_good + unacc_good + vgood_good))\n",
    "Precision_of_good = good_good / (acc_good + good_good + unacc_good + vgood_good)\n",
    "print(f\"Precision of good: {Precision_of_good}\")\n",
    "\n",
    "# Precision of unacc\n",
    "# print(unacc_unacc / (acc_unacc + good_unacc + unacc_unacc + vgood_unacc))\n",
    "Precision_of_unacc = unacc_unacc / (acc_unacc + good_unacc + unacc_unacc + vgood_unacc)\n",
    "print(f\"Precision of unacc: {Precision_of_unacc}\")\n",
    "\n",
    "# Precision of vgood\n",
    "# print(vgood_vgood / (acc_vgood + good_vgood + unacc_vgood + vgood_vgood))\n",
    "Precision_of_vgood = vgood_vgood / (acc_vgood + good_vgood + unacc_vgood + vgood_vgood)\n",
    "print(f\"Precision of vgood: {Precision_of_vgood}\")\n",
    "\n",
    "# Average precision\n",
    "average_precision = (Precision_of_acc + Precision_of_good + Precision_of_unacc + Precision_of_vgood) / 4.0\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"Average Precision: {average_precision}\")\n",
    "print(\"---------------------------------------\")\n",
    "\n",
    "# Recall of acc\n",
    "Recall_of_acc = acc_acc / (acc_acc + acc_good + acc_unacc + acc_vgood)\n",
    "print(f\"Recall of acc: {Recall_of_acc}\")\n",
    "\n",
    "# Recall of good\n",
    "Recall_of_good = good_good / (good_acc + good_good + good_unacc + good_vgood)\n",
    "print(f\"Recall of good: {Recall_of_good}\")\n",
    "\n",
    "# Recall of unacc\n",
    "Recall_of_unacc = unacc_unacc / (unacc_acc + unacc_good + unacc_unacc + unacc_vgood)\n",
    "print(f\"Recall of unacc: {Recall_of_unacc}\")\n",
    "\n",
    "# Recall of vgood\n",
    "Recall_of_vgood = vgood_vgood / (vgood_acc + vgood_good + vgood_unacc + vgood_vgood)\n",
    "print(f\"Recall of vgood: {Recall_of_vgood}\")\n",
    "\n",
    "# Average recall\n",
    "average_recall = (Recall_of_acc + Recall_of_good + Recall_of_unacc + Recall_of_vgood) / 4.0\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"Average recall: {average_recall}\")\n",
    "print(\"---------------------------------------\")\n",
    "\n",
    "# Average F1 Score\n",
    "average_f1_score = 2 * ((average_precision * average_recall) / (average_precision + average_recall))\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"Average F1 Score: {average_f1_score}\")\n",
    "print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08d9259-f108-4137-b3ee-ea6086839150",
   "metadata": {},
   "source": [
    "__Confusion Matrix, Precision, Recall, F1 Score on the Test Set__ `using the Sklearn Implementation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0f8a1b-ae5d-4557-b4d1-610f3ae5c4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "y_test_labels = np.argmax(y_test.values, axis=1)\n",
    "y_test_pred_labels = np.argmax(y_test_pred, axis=1)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_labels, y_test_pred_labels)\n",
    "\n",
    "print(\"Confusion Matrix using sklearn:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_labels, y_test_pred_labels, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c958b2c8-94e7-4c2d-b097-d6104432f5cf",
   "metadata": {},
   "source": [
    "__XGBOOST__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e92659-0004-43b1-8302-99e21cc81047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support, precision_recall_curve, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create an XGBoost Classifier\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_val = xgb_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_val, y_pred_val, average=None)\n",
    "print(\"Validation Precision:\", precision)\n",
    "print(\"Validation Recall:\", recall)\n",
    "print(\"Validation F1-score:\", f1)\n",
    "print(\"Validation Support:\", support)\n",
    "\n",
    "# # Precision-Recall Curve for validation set\n",
    "# precision_curve, recall_curve, thresholds = precision_recall_curve(y_val, xgb_model.predict_proba(X_val)[:, 1])\n",
    "# plt.plot(recall_curve, precision_curve)\n",
    "# plt.xlabel('Recall')\n",
    "# plt.ylabel('Precision')\n",
    "# plt.title('Precision-Recall Curve (Validation)')\n",
    "# plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred_test = xgb_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred_test, average=None)\n",
    "print(\"Test Precision:\", precision)\n",
    "print(\"Test Recall:\", recall)\n",
    "print(\"Test F1-score:\", f1)\n",
    "print(\"Test Support:\", support)\n",
    "\n",
    "# Confusion Matrix for test set\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# # Precision-Recall Curve for test set\n",
    "# precision_curve, recall_curve, thresholds = precision_recall_curve(y_test, xgb_model.predict_proba(X_test)[:, 1])\n",
    "# plt.plot(recall_curve, precision_curve)\n",
    "# plt.xlabel('Recall')\n",
    "# plt.ylabel('Precision')\n",
    "# plt.title('Precision-Recall Curve (Test)')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3924c333-4df2-4636-9308-0abd318410dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_model = XGBClassifier(colsample_bytree = 1, n_estimators=200, learning_rate=0.1, max_depth=11, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "f1_xgb = f1_score(y_test, y_pred_xgb, average='weighted')\n",
    "precision_xgb = precision_score(y_test, y_pred_xgb, average='weighted')\n",
    "recall_xgb = recall_score(y_test, y_pred_xgb, average='weighted')\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(\"XGBoost Model Performance:\")\n",
    "print(f\"F1 Score: {f1_xgb}\")\n",
    "print(f\"Precision: {precision_xgb}\")\n",
    "print(f\"Recall: {recall_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e54cff6-c292-40f0-897b-2b1c133cfe56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
